{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee8dc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@10b7d80c\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "  .master(\"local\")\n",
    "  .appName(\"schiphol\")\n",
    "  // .config(\"spark.some.config.option\", \"some-value\")\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8eef8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "// // visualize dataframes\n",
    "// // https://github.com/almond-sh/almond/issues/180\n",
    "\n",
    "// import org.apache.spark.sql._\n",
    "\n",
    "// implicit class RichDF(val df: DataFrame) {\n",
    "//     def view(limit:Int = 20, truncate: Int = 20) = {\n",
    "//         import xml.Utility.escape\n",
    "//         val data = df.take(limit)\n",
    "//         val header = df.schema.fieldNames.toSeq        \n",
    "//         val rows: Seq[Seq[String]] = data.map { row =>\n",
    "//           row.toSeq.map { cell =>\n",
    "//             val str = cell match {\n",
    "//               case null => \"null\"\n",
    "//               case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\")\n",
    "//               case array: Array[_] => array.mkString(\"[\", \", \", \"]\")\n",
    "//               case seq: Seq[_] => seq.mkString(\"[\", \", \", \"]\")\n",
    "//               case _ => cell.toString\n",
    "//             }\n",
    "//             if (truncate > 0 && str.length > truncate) {\n",
    "//               // do not show ellipses for strings shorter than 4 characters.\n",
    "//               if (truncate < 4) str.substring(0, truncate)\n",
    "//               else str.substring(0, truncate - 3) + \"...\"\n",
    "//             } else {\n",
    "//               str\n",
    "//             }\n",
    "//           }: Seq[String]\n",
    "//         }\n",
    "\n",
    "//         kernel.display.html(s\"\"\" <table>\n",
    "//                 <tr>\n",
    "//                  ${header.map(h => s\"<th>${escape(h)}</th>\").mkString}\n",
    "//                 </tr>\n",
    "//                 ${rows.map { row =>\n",
    "//                   s\"<tr>${row.map{c => s\"<td>${escape(c)}</td>\" }.mkString}</tr>\"\n",
    "//                 }.mkString}\n",
    "//             </table>\n",
    "//         \"\"\")        \n",
    "//     }\n",
    "// }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e00b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Use tooling you think is best fitted for the task (e.g. Docker, public cloud, etc.)\n",
    "// and give us an explanation why you decided for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ce0654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------------+-----------+-------------+---------+-----+---------+\n",
      "|airline|airlineId|srcAirport|srcAirportId|destAirport|destAirportId|codeshare|stops|equipment|\n",
      "+-------+---------+----------+------------+-----------+-------------+---------+-----+---------+\n",
      "|     2B|      410|       AER|        2965|        KZN|         2990|     null|    0|      CR2|\n",
      "|     2B|      410|       ASF|        2966|        KZN|         2990|     null|    0|      CR2|\n",
      "|     2B|      410|       ASF|        2966|        MRV|         2962|     null|    0|      CR2|\n",
      "|     2B|      410|       CEK|        2968|        KZN|         2990|     null|    0|      CR2|\n",
      "|     2B|      410|       CEK|        2968|        OVB|         4078|     null|    0|      CR2|\n",
      "|     2B|      410|       DME|        4029|        KZN|         2990|     null|    0|      CR2|\n",
      "|     2B|      410|       DME|        4029|        NBC|         6969|     null|    0|      CR2|\n",
      "|     2B|      410|       DME|        4029|        TGK|         null|     null|    0|      CR2|\n",
      "|     2B|      410|       DME|        4029|        UUA|         6160|     null|    0|      CR2|\n",
      "|     2B|      410|       EGO|        6156|        KGD|         2952|     null|    0|      CR2|\n",
      "+-------+---------+----------+------------+-----------+-------------+---------+-----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.catalyst.ScalaReflection\n",
       "import spark.implicits._\n",
       "cols: Seq[String] = List(airline, airlineId, srcAirport, srcAirportId, destAirport, destAirportId, codeshare, stops, equipment)\n",
       "defined class FlightRoute\n",
       "df1: org.apache.spark.sql.DataFrame = [airline: string, airlineId: string ... 7 more fields]\n",
       "df2: org.apache.spark.sql.DataFrame = [airline: string, airlineId: string ... 7 more fields]\n",
       "numericCols: Seq[String] = List(airlineId, srcAirportId, destAirportId, stops)\n",
       "df3: org.apache.spark.sql.DataFrame = [airline: string, airlineId: int ... 7 more fields]\n",
       "ds: org.apache.spark.sql.Dataset[FlightRoute] = [airline: string, airlineId: int ... 7 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Using the flight routes dataset you will stream the given data into our pipeline, process it and display\n",
    "// results. Keep in mind the resulting implementation needs to be somehow demoable.\n",
    "\n",
    "import org.apache.spark.sql.catalyst.ScalaReflection\n",
    "import spark.implicits._\n",
    "\n",
    "// columns as described in https://openflights.org/data.html\n",
    "val cols = Seq(\"airline\", \"airlineId\", \"srcAirport\", \"srcAirportId\", \"destAirport\", \"destAirportId\", \"codeshare\", \"stops\", \"equipment\")\n",
    "// case class FlightRoute(\n",
    "//     airline: String, // 2-letter (IATA) or 3-letter (ICAO) code of the airline.\n",
    "//     airlineId: Option[String], // Unique OpenFlights identifier for airline.\n",
    "//     srcAirport: String, // 3-letter (IATA) or 4-letter (ICAO) code of the source airport.\n",
    "//     srcAirportId: Option[String], // Unique OpenFlights identifier for source airport.\n",
    "//     destAirport: String, // 3-letter (IATA) or 4-letter (ICAO) code of the destination airport.\n",
    "//     destAirportId: Option[String], // Unique OpenFlights identifier for destination airport.\n",
    "//     codeshare: Option[String], // \"Y\" if this flight is a codeshare (that is, not operated by Airline, but another carrier), empty otherwise.\n",
    "//     stops: String, // Number of stops on this flight (\"0\" for direct)\n",
    "//     equipment: Option[String], // 3-letter codes for plane type(s) generally used on this flight, separated by spaces\n",
    "// )\n",
    "case class FlightRoute(\n",
    "    airline: String, // 2-letter (IATA) or 3-letter (ICAO) code of the airline.\n",
    "    airlineId: Option[Int], // Unique OpenFlights identifier for airline.\n",
    "    srcAirport: String, // 3-letter (IATA) or 4-letter (ICAO) code of the source airport.\n",
    "    srcAirportId: Option[Int], // Unique OpenFlights identifier for source airport.\n",
    "    destAirport: String, // 3-letter (IATA) or 4-letter (ICAO) code of the destination airport.\n",
    "    destAirportId: Option[Int], // Unique OpenFlights identifier for destination airport.\n",
    "    codeshare: Option[String], // \"Y\" if this flight is a codeshare (that is, not operated by Airline, but another carrier), empty otherwise.\n",
    "    stops: Int, // Number of stops on this flight (\"0\" for direct)\n",
    "    equipment: Option[String], // 3-letter codes for plane type(s) generally used on this flight, separated by spaces\n",
    ")\n",
    "\n",
    "// val encoder = org.apache.spark.sql.Encoders.product[FlightRoute]\n",
    "\n",
    "// val schema = StructType( Seq (\n",
    "//     StructField( \"airline\", StringType, false),\n",
    "//     StructField( \"airlineId\", StringType, true),\n",
    "//     StructField( \"srcAirport\", StringType, false),\n",
    "//     StructField( \"srcAirportId\", StringType, true),\n",
    "//     StructField( \"destAirport\", StringType, false),\n",
    "//     StructField( \"destAirportId\", StringType, true),\n",
    "//     StructField( \"codeshare\", StringType, true),\n",
    "//     StructField( \"stops\", StringType, false),\n",
    "//     StructField( \"equipment\", StringType, true),\n",
    "// ))\n",
    "\n",
    "val df1 = spark.read\n",
    "// val dataset = spark.read\n",
    "//     .format(\"csv\")\n",
    "    .option(\"header\",\"false\")\n",
    "    // .schema(schema)\n",
    "    .csv(\"file:///home/jovyan/routes.dat\")\n",
    "    .toDF(cols: _*)\n",
    "//     .as[FlightRoute]\n",
    "\n",
    "// write to folder to use later as stream\n",
    "df1.write.mode(\"overwrite\").option(\"header\",false).csv(\"file:///home/jovyan/out/routes\")\n",
    "\n",
    "// The special value \\N is used for \"NULL\" to indicate that no value is available\n",
    "val df2 = cols.foldLeft(df1)((df: DataFrame, column: String) => df.withColumn(column, when(col(column) === \"\\\\N\",lit(null)).otherwise(col(column))))\n",
    "// cast numeric columns to int\n",
    "val numericCols = Seq(\"airlineId\", \"srcAirportId\", \"destAirportId\", \"stops\")\n",
    "val df3 = numericCols.foldLeft(df2)((df: DataFrame, column: String) => df.withColumn(column,col(column).cast(IntegerType)))\n",
    "\n",
    "// val dataset = df.as(encoder)\n",
    "\n",
    "// val schema = ScalaReflection.schemaFor[FlightRoute].dataType.asInstanceOf[StructType]\n",
    "// val newSchema = StructType(schema.map {\n",
    "//   case StructField( c, t, _, m) â‡’ StructField( c, t, nullable = false, m)\n",
    "// })\n",
    "// val dataset = spark.createDataFrame(df.rdd, schema).as[FlightRoute]\n",
    "val ds = df3.as[FlightRoute]\n",
    "\n",
    "ds.show(10) // 100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409163c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab03820",
   "metadata": {},
   "outputs": [],
   "source": [
    "// // val routeTable = (ds: Dataset[FlightRoute]): String =>\n",
    "// def routeTable(ds: Dataset[FlightRoute]) =\n",
    "//       records.map {\n",
    "//         case FlightRoute(name, age, gender)\n",
    "//           s\"<tr><td>airline}</td><td>${airlineId}</td><td>${srcAirport}</td><td>${srcAirportId}</td><td>${destAirport}</td><td>${destAirportId}</td><td>${codeshare}</td><td>${stops}</td><td>${equipment</td></tr>\"\n",
    "//       }.mkString(\n",
    "//         \"<table><tr><th>airline</th><th>airlineId</th><th>srcAirport</th><th>srcAirportId</th><th>destAirport</th><th>destAirportId</th><th>codeshare</th><th>stops</th><th>equipment</th></tr>\",\n",
    "//         \"\",\n",
    "//         \"</table>\")\n",
    "\n",
    "// routeTable(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb51895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|srcAirport|count|\n",
      "+----------+-----+\n",
      "|       ATL|  915|\n",
      "|       ORD|  558|\n",
      "|       PEK|  535|\n",
      "|       LHR|  527|\n",
      "|       CDG|  524|\n",
      "|       FRA|  497|\n",
      "|       LAX|  492|\n",
      "|       DFW|  469|\n",
      "|       JFK|  456|\n",
      "|       AMS|  453|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "top10: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [srcAirport: string, count: bigint]\n",
       "ds_schema: org.apache.spark.sql.types.StructType = StructType(StructField(airline,StringType,true), StructField(airlineId,IntegerType,true), StructField(srcAirport,StringType,true), StructField(srcAirportId,IntegerType,true), StructField(destAirport,StringType,true), StructField(destAirportId,IntegerType,true), StructField(codeshare,StringType,true), StructField(stops,IntegerType,true), StructField(equipment,StringType,true))\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a batch Spark job that read in the routes dataset.\n",
    "// It should create an overview of the top 10 airports used as source airport.\n",
    "val top10 = ds.groupBy(\"srcAirport\").count().sort(col(\"count\").desc).limit(10)\n",
    "top10.show()\n",
    "// Write the output to a filesystem.\n",
    "ds.write.mode(\"overwrite\").option(\"header\",true)\n",
    "   .csv(\"file:///home/jovyan/out/routes.csv\")\n",
    "val ds_schema = ds.schema\n",
    "top10.write.mode(\"overwrite\").option(\"header\",true)\n",
    "   .csv(\"file:///home/jovyan/out/top10.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d885a9d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.streaming.StreamingQueryException",
     "evalue": " Query [id = 93856bc0-2ee5-4222-af42-68790491a5a3, runId = 8c5cc0c4-24df-4efe-a32e-e80d43f1bd29] terminated with exception: Wrong basePath file:/home/jovyan/out/routes for the root path: file:/home/jovyan/routes/part-00000-48828e63-bd5f-4ae9-bf16-10947c6b2c55-c000.csv",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.streaming.StreamingQueryException: Query [id = 93856bc0-2ee5-4222-af42-68790491a5a3, runId = 8c5cc0c4-24df-4efe-a32e-e80d43f1bd29] terminated with exception: Wrong basePath file:/home/jovyan/out/routes for the root path: file:/home/jovyan/routes/part-00000-48828e63-bd5f-4ae9-bf16-10947c6b2c55-c000.csv",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:354)",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)",
      "Caused by: java.lang.IllegalArgumentException: Wrong basePath file:/home/jovyan/out/routes for the root path: file:/home/jovyan/routes/part-00000-48828e63-bd5f-4ae9-bf16-10947c6b2c55-c000.csv",
      "  at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.$anonfun$basePaths$3(PartitioningAwareFileIndex.scala:230)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.basePaths(PartitioningAwareFileIndex.scala:228)",
      "  at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:154)",
      "  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:73)",
      "  at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:50)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:418)",
      "  at org.apache.spark.sql.execution.streaming.FileStreamSource.getBatch(FileStreamSource.scala:231)",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$populateStartOffsets$3(MicroBatchExecution.scala:314)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:941)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:941)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:27)",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.populateStartOffsets(MicroBatchExecution.scala:311)",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:197)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357)",
      "  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355)",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:194)",
      "  at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:188)",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:333)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "// Use Spark structured streaming to change your job into a streaming job, and use the dataset file\n",
    "// as a source.\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.streaming._\n",
    "import org.apache.spark.streaming.dstream._\n",
    "import org.apache.spark.sql.streaming._\n",
    "\n",
    "val schema = StructType( Seq (\n",
    "    StructField(\"airline\", StringType, false),\n",
    "    StructField(\"airlineId\", StringType, true),\n",
    "    StructField(\"srcAirport\", StringType, false),\n",
    "    StructField(\"srcAirportId\", StringType, true),\n",
    "    StructField(\"destAirport\", StringType, false),\n",
    "    StructField(\"destAirportId\", StringType, true),\n",
    "    StructField(\"codeshare\", StringType, true),\n",
    "    StructField(\"stops\", StringType, false),\n",
    "    StructField(\"equipment\", StringType, true),\n",
    "))\n",
    "\n",
    "val stream_df1 = spark.readStream\n",
    "    .option(\"header\",\"true\")\n",
    "    .schema(schema)\n",
    "    .csv(\"file:///home/jovyan/out/routes\")\n",
    "\n",
    "// // The special value \\N is used for \"NULL\" to indicate that no value is available\n",
    "// val stream_df2 = cols.foldLeft(stream_df1)((df: DataFrame, column: String) => df.withColumn(column, when(col(column) === \"\\\\N\",lit(null)).otherwise(col(column))))\n",
    "// // cast numeric columns to int\n",
    "// val numericCols = Seq(\"airlineId\", \"srcAirportId\", \"destAirportId\", \"stops\")\n",
    "// val stream_df3 = numericCols.foldLeft(stream_df2)((df: DataFrame, column: String) => df.withColumn(column,col(column).cast(IntegerType)))\n",
    "// val stream_ds = stream_df3.as[FlightRoute]\n",
    "\n",
    "stream_df1\n",
    "// stream_ds\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/\")\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    "\n",
    "// val top10 = ds.groupBy(\"srcAirport\").count().sort(col(\"count\").desc).limit(10)\n",
    "// top10.write.mode(\"overwrite\").option(\"header\",true)\n",
    "//    .csv(\"file:///home/jovyan/out/top10_stream.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0fabed",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Next change your streaming job so the aggregations are done using sliding windows. Pick any\n",
    "// window and sliding interval. The end result should be the top 10 airports used as source airport\n",
    "// within each window. When choosing the window interval, keep the size of the dataset in mind.\n",
    "\n",
    ".groupBy(\n",
    "  window($\"timestamp\", \"10 minutes\", \"5 minutes\"),\n",
    "  $\"word\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Productionize your code by adding unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e95596",
   "metadata": {},
   "outputs": [],
   "source": [
    "// description of your steps and thinking process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
